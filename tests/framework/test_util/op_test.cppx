/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#include "op_test.h"

#include <fstream>
#include <iterator>
#include <random>
#include <securec.h>
#include <sstream>

#include "mki/ops.h"
#include "mki/utils/assert/assert.h"
#include "mki/utils/fp16/fp16_t.h"
#include "mki/utils/log/log.h"
#include "mki/utils/rt/rt.h"
#include "mki/utils/time/timer.h"
#include "mki/params/params.h"
#include "float_util.h"

namespace Mki {
constexpr float ATOL = 0.00001;
constexpr float RTOL = 0.00001;

std::string OpTestStatistic::ToString() const
{
    std::stringstream ss;
    ss << "total:" << total << ", init:" << init << ", prepare:" << prepare << ", kernelRun:" << kernelRun
       << ", streamSync:" << streamSync << ", runGolden:" << runGolden;
    return ss.str();
}

OpTest::OpTest() : launchWithTiling_(true), deviceId_(0) {}

OpTest::~OpTest()
{
    Cleanup();
    int ret = MkiRtDeviceResetCurrent(deviceId_);
    MKI_LOG_IF(ret != 0, ERROR) << "MkiRtDeviceResetCurrent fail";
    delete op_;
}

void OpTest::Golden(OpTestGolden golden) { golden_ = golden; }

void OpTest::FloatRand(float min, float max)
{
    randFloatMin_ = min;
    randFloatMax_ = max;
    MKI_LOG(INFO) << "randFloatMin:" << randFloatMin_ << ", randFloatMax:" << randFloatMax_;
}

void OpTest::Int8Rand(int8_t min, int8_t max)
{
    randInt8Min_ = min;
    randInt8Max_ = max;
    MKI_LOG(INFO) << "randIntMin:" << randInt8Min_ << ", randIntMax:" << randInt8Max_;
}

void OpTest::IntRand(int32_t min, int32_t max)
{
    randIntMin_ = min;
    randIntMax_ = max;
    MKI_LOG(INFO) << "randIntMin:" << randIntMin_ << ", randIntMax:" << randIntMax_;
}

void OpTest::LongRand(int64_t min, int64_t max)
{
    randLongMin_ = min;
    randLongMax_ = max;
    MKI_LOG(INFO) << "randIntMin:" << randLongMin_ << ", randIntMax:" << randLongMax_;
}

Status OpTest::Run(const UtOpDesc &opDesc, const TensorDesc &inTensorDesc, const TensorDesc &outTensorDesc,
                   const std::string &opName)
{
    SVector<TensorDesc> inTensorDescs = {inTensorDesc};
    SVector<TensorDesc> outTensorDescs = {outTensorDesc};
    return Run(opDesc, inTensorDescs, outTensorDescs, opName);
}

Status OpTest::Run(const UtOpDesc &opDesc, const SVector<TensorDesc> &inTensorDescs,
                   const SVector<TensorDesc> &outTensorDescs, const std::string &opName)
{
    Timer timer;
    Status status = RunImpl(opDesc, inTensorDescs, outTensorDescs, opName);
    statistic_.total = timer.ElapsedMicroSecond();
    MKI_LOG(INFO) << "statistic:" << statistic_.ToString();
    return status;
}

Status OpTest::Run(const UtOpDesc &opDesc, const SVector<Tensor> &inTensorLists, const SVector<Tensor> &outTensorLists,
                   const std::string &opName)
{
    Timer timer;
    Status status = RunImpl(opDesc, inTensorLists, outTensorLists, opName);
    statistic_.total = timer.ElapsedMicroSecond();
    MKI_LOG(INFO) << "statistic:" << statistic_.ToString();
    return status;
}

Status OpTest::RunWithDataFile(const UtOpDesc &opDesc, const SVector<TensorDesc> &inTensorDescs,
                               const SVector<TensorDesc> &outTensorDescs, const SVector<std::string> &files,
                               const std::string &opName)
{
    Timer timer;
    dataFiles_ = files;
    Status status = RunImpl(opDesc, inTensorDescs, outTensorDescs, opName);
    statistic_.total = timer.ElapsedMicroSecond();
    MKI_LOG(INFO) << "statistic:" << statistic_.ToString();
    return status;
}

Status OpTest::RunImpl(const UtOpDesc &opDesc, const SVector<TensorDesc> &inTensorDescs,
                       const SVector<TensorDesc> &outTensorDescs, const std::string &opName)
{
    Timer timer;
    Cleanup();
    Init();
    statistic_.init = timer.ElapsedMicroSecond();

    timer.Reset();
    Status status = Prepare(opDesc, inTensorDescs, outTensorDescs);
    statistic_.prepare = timer.ElapsedMicroSecond();
    if (!status.Ok()) {
        return status;
    }

    if (opName.compare("") == 0) {
        status = RunKernel(opDesc.opName);
    } else {
        status = RunKernel(opName);
    }
    
    if (!status.Ok()) {
        return status;
    }

    status = CopyDeviceTensorToHostTensor();
    if (!status.Ok()) {
        return status;
    }

    timer.Reset();
    status = RunGolden();
    statistic_.runGolden = timer.ElapsedMicroSecond();
    if (!status.Ok()) {
        MKI_LOG(ERROR) << "golden fail, error:" << status.ToString();
        return status;
    }
    return Status::OkStatus();
}

Status OpTest::RunImpl(const UtOpDesc &opDesc, const SVector<Tensor> &inTensorLists,
                       const SVector<Tensor> &outTensorLists, const std::string &opName)
{
    Timer timer;
    Cleanup();
    Init();
    statistic_.init = timer.ElapsedMicroSecond();

    timer.Reset();
    Status status = Prepare(opDesc, inTensorLists, outTensorLists);
    statistic_.prepare = timer.ElapsedMicroSecond();
    if (!status.Ok()) {
        return status;
    }

    if (opName.compare("") == 0) {
        status = RunKernel(opDesc.opName);
    } else {
        status = RunKernel(opName);
    }
    if (!status.Ok()) {
        return status;
    }

    status = CopyDeviceTensorToHostTensor();
    if (!status.Ok()) {
        return status;
    }

    timer.Reset();
    status = RunGolden();
    statistic_.runGolden = timer.ElapsedMicroSecond();
    if (!status.Ok()) {
        MKI_LOG(ERROR) << "golden fail, error:" << status.ToString();
        return status;
    }
    return Status::OkStatus();
}

Status OpTest::Prepare(const UtOpDesc &opDesc, const SVector<TensorDesc> &inTensorDescs,
                       const SVector<TensorDesc> &outTensorDescs)
{
    op_ = Ops::Instance().GetKernelInstance(opDesc.opName);
    if (op_ == nullptr) {
        MKI_LOG(ERROR) << opDesc.opName << " not exists";
        return Status::FailStatus(-1, opDesc.opName + " not exists");
    }

    goldenContext_.opDesc = opDesc;
    launchParam_.SetParam(opDesc.specificParam);

    for (auto &tensorDesc : inTensorDescs) {
        Tensor tensor = {tensorDesc, nullptr, 0};
        launchParam_.AddInTensor(tensor);
    }

    for (auto &tensorDesc : outTensorDescs) {
        Tensor tensor = {tensorDesc, nullptr, 0};
        launchParam_.AddOutTensor(tensor);
    }

    MallocInTensor();

    MallocOutTensor();

    return Status::OkStatus();
}

Status OpTest::Prepare(const UtOpDesc &opDesc, const SVector<Tensor> &inTensorLists,
                       const SVector<Tensor> &outTensorLists)
{
    op_ = Ops::Instance().GetOperationByName(opDesc.opName);
    if (op_ == nullptr) {
        MKI_LOG(ERROR) << opDesc.opName << " not exists";
        return Status::FailStatus(-1, opDesc.opName + " not exists");
    }

    goldenContext_.opDesc = opDesc;
    launchParam_.SetParam(opDesc.specificParam);

    for (auto tensor : inTensorLists) {
        launchParam_.AddInTensor(tensor);
    }

    for (auto tensor : outTensorLists) {
        launchParam_.AddOutTensor(tensor);
    }

    MallocInTensor(inTensorLists);

    Status status = op_->InferShape(launchParam_);
    if (!status.Ok()) {
        MKI_LOG(ERROR) << opDesc.opName << " infer shape fail, error:" << status.ToString();
        return status;
    }

    MallocOutTensor(outTensorLists);

    if (!op_->IsConsistent(launchParam_)) {
        MKI_LOG(ERROR) << op_->GetName() << " check launchParam is consistent fail";
        return Status::FailStatus(-1, op_->GetName() + " check launchParam is consistent fail");
    }

    return Status::OkStatus();
}

void OpTest::MallocInTensor()
{
    for (size_t i = 0; i < launchParam_.GetInTensorCount(); ++i) {
        Tensor &deviceTensor = launchParam_.GetInTensor(i);
        Tensor hostTensor;
        if (dataFiles_.size() > i) {
            hostTensor = CreateHostTensorFromFile(deviceTensor.desc, dataFiles_[i]);
        } else {
            hostTensor = CreateHostRandTensor(deviceTensor.desc);
        }
        deviceTensor.dataSize = hostTensor.dataSize;
        deviceTensor = HostTensor2DeviceTensor(hostTensor);
        MKI_LOG(INFO) << "InTensor[" << i << "] = " << TensorToString(hostTensor);
        goldenContext_.hostInTensors.push_back(hostTensor);
        goldenContext_.deviceInTensors.push_back(deviceTensor);
    }
}

void OpTest::MallocInTensor(const SVector<Tensor> &inTensorLists)
{
    if (launchParam_.GetInTensorCount() != inTensorLists.size()) {
        MKI_LOG(ERROR) << "MallocInTensor ERROR";
        return;
    }
    for (size_t i = 0; i < launchParam_.GetInTensorCount(); ++i) {
        Tensor &deviceTensor = launchParam_.GetInTensor(i);
        Tensor hostTensor = CreateHostTensor(inTensorLists.at(i));
        deviceTensor.dataSize = hostTensor.dataSize;
        deviceTensor = HostTensor2DeviceTensor(hostTensor);
        MKI_LOG(INFO) << "InTensor[" << i << "] = " << TensorToString(hostTensor);
        goldenContext_.hostInTensors.push_back(hostTensor);
        goldenContext_.deviceInTensors.push_back(deviceTensor);
    }
}

void OpTest::MallocOutTensor()
{
    for (size_t i = 0; i < launchParam_.GetOutTensorCount(); ++i) {
        Tensor &deviceTensor = launchParam_.GetOutTensor(i);
        Tensor hostTensor = CreateHostZeroTensor(deviceTensor.desc);
        deviceTensor.dataSize = hostTensor.dataSize;
        if (outUseInputdata_.find(i) != outUseInputdata_.end()) {
            deviceTensor.data = launchParam_.GetInTensor(outUseInputdata_[i]).data;
        } else {
            int st = MkiRtMemMallocDevice(&deviceTensor.data, deviceTensor.dataSize, MKIRT_MEM_DEFAULT);
            MKI_LOG_IF(st != MKIRT_SUCCESS, ERROR) << "malloc device tensor fail";

            // init deviceTensor zero
            st = MkiRtMemCopy(deviceTensor.data, deviceTensor.dataSize, hostTensor.data, hostTensor.dataSize,
                              MKIRT_MEMCOPY_HOST_TO_DEVICE);
            MKI_LOG_IF(st != MKIRT_SUCCESS, ERROR) << "init device tensor 0 fail";
        }
        goldenContext_.hostOutTensors.push_back(hostTensor);
        goldenContext_.deviceOutTensors.push_back(deviceTensor);
    }
}

void OpTest::MallocOutTensor(const SVector<Tensor> &outTensorLists)
{
    if (launchParam_.GetOutTensorCount() != outTensorLists.size()) {
        MKI_LOG(ERROR) << "MallocOutTensor ERROR";
        return;
    }
    for (size_t i = 0; i < launchParam_.GetOutTensorCount(); ++i) {
        Tensor &deviceTensor = launchParam_.GetOutTensor(i);
        Tensor hostTensor = CreateHostTensor(outTensorLists.at(i));
        deviceTensor.dataSize = hostTensor.dataSize;
        deviceTensor = HostTensor2DeviceTensor(hostTensor);
        MKI_LOG(INFO) << "OutTensor[" << i << "] = " << TensorToString(hostTensor);
        goldenContext_.hostOutTensors.push_back(hostTensor);
        goldenContext_.deviceOutTensors.push_back(deviceTensor);
    }
}

void OpTest::SetOutdataUseInputData(size_t outIndex, size_t inIndex) { outUseInputdata_[outIndex] = inIndex; }

Status OpTest::RunKernel(const std::string &opName)
{
    uint8_t *deviceLaunchBuffer = nullptr;

    if (launchWithTiling_) {
        op_->SetLaunchWithTiling(true);
        auto status = op_->Init(launchParam_);
        MKI_CHECK(status.Ok(), "failed to init op", return Status::FailStatus(-1, "failed to init op"));
    } else {
        op_->SetLaunchWithTiling(false);
        uint32_t launchBufferSize = op_->GetTilingSize(launchParam_);
        MKI_CHECK(launchBufferSize > 0, "empty tiling size", return Status::FailStatus(-1, "empty tiling size"));
        uint8_t hostLaunchBuffer[launchBufferSize];
        op_->SetTilingHostAddr(hostLaunchBuffer, launchBufferSize);
        auto status = op_->Init(launchParam_);
        MKI_CHECK(status.Ok(), "failed to init op", return Status::FailStatus(-1, "failed to init op"));

        int st = MkiRtMemMallocDevice(reinterpret_cast<void **>(&deviceLaunchBuffer),
                                      launchBufferSize, MKIRT_MEM_DEFAULT);
        MKI_CHECK(st == MKIRT_SUCCESS, "malloc device memory fail",
                  return Status::FailStatus(-1, "malloc device memory fail"));
        st = MkiRtMemCopy(deviceLaunchBuffer, launchBufferSize,
            hostLaunchBuffer, launchBufferSize, MKIRT_MEMCOPY_HOST_TO_DEVICE);
        if (st != MKIRT_SUCCESS) {
            MkiRtMemFreeDevice(deviceLaunchBuffer);
            deviceLaunchBuffer = nullptr;
            MKI_LOG(ERROR) << "copy host memory to device fail";
            return Status::FailStatus(-1, "copy host memory to device fail");
        }
        runInfo_.SetTilingDeviceAddr(deviceLaunchBuffer);
    }

    AddWorkspace();

    MKI_LOG(INFO) << op_->GetName() << " run start, LaunchParam:\n" << launchParam_.ToString();
    MKI_LOG(INFO) << "RunInfo:\n" << runInfo_.ToString();
    Timer timer;
    Status status = op_->Run(launchParam_, runInfo_);
    statistic_.kernelRun = timer.ElapsedMicroSecond();
    if (status.Ok()) {
        MKI_LOG(INFO) << op_->GetName() << " run success";
    } else {
        MKI_LOG(ERROR) << op_->GetName() << " run fail, error:" << status.ToString();
    }

    timer.Reset();
    int ret = MkiRtStreamSynchronize(runInfo_.GetStream());
    statistic_.streamSync = timer.ElapsedMicroSecond();
    MKI_LOG(INFO) << "sync ret " << ret;
    MKI_LOG_IF(ret != 0, ERROR) << "MkiRtStreamSynchronize fail";

    if (deviceLaunchBuffer != nullptr) {
        MkiRtMemFreeDevice(deviceLaunchBuffer);
    }

    FreeWorkspace();

    return Status::OkStatus();
}

Status OpTest::CopyDeviceTensorToHostTensor()
{
    for (size_t i = 0; i < launchParam_.GetOutTensorCount(); ++i) {
        Tensor &deivceTensor = launchParam_.GetOutTensor(i);
        Tensor &hostTensor = goldenContext_.hostOutTensors.at(i);
        MKI_LOG(INFO) << "MkiRtMemCopy start, hostTensor.data:" << hostTensor.data
                      << ", hostTensor.dataSize:" << hostTensor.dataSize << ", deivceTensor.data:" << deivceTensor.data
                      << ", deivceTensor.dataSize:" << deivceTensor.dataSize;
        int st = MkiRtMemCopy(hostTensor.data, hostTensor.dataSize, deivceTensor.data, deivceTensor.dataSize,
                              MKIRT_MEMCOPY_DEVICE_TO_HOST);
        if (st != 0) {
            MKI_LOG(ERROR) << "copy memory from device to host fail";
            return Status::FailStatus(-1, "copy memory from device to host fail");
        }
        MKI_LOG(INFO) << "OutTensor[" << i << "] = " << TensorToString(hostTensor);
    }
    return Status::OkStatus();
}

void OpTest::SetOutputNum(int64_t outputNum) { outputNum_ = outputNum; }

int64_t OpTest::GetOutputNum() { return outputNum_; }

Status OpTest::RunGolden()
{
    if (golden_) {
        return golden_(goldenContext_);
    }
    return Status::OkStatus();
}

Tensor OpTest::CreateHostRandTensor(const TensorDesc &tensorDesc)
{
    Tensor tensor;
    tensor.desc = tensorDesc;

    std::random_device rd;
    std::default_random_engine eng(rd());
    if (tensorDesc.dtype == TENSOR_DTYPE_FLOAT) {
        tensor.dataSize = tensor.Numel() * sizeof(float);
        tensor.data = malloc(tensor.dataSize);
        std::uniform_real_distribution<float> distr(randFloatMin_, randFloatMax_);
        float *tensorData = static_cast<float *>(tensor.data);
        for (int64_t i = 0; i < tensor.Numel(); i++) {
            tensorData[i] = static_cast<float>(distr(eng));
        }
    } else if (tensorDesc.dtype == TENSOR_DTYPE_FLOAT16) {
        tensor.dataSize = tensor.Numel() * sizeof(fp16_t);
        tensor.data = malloc(tensor.dataSize);
        std::uniform_real_distribution<float> distr(randFloatMin_, randFloatMax_);
        fp16_t *tensorData = static_cast<fp16_t *>(tensor.data);
        for (int64_t i = 0; i < tensor.Numel(); i++) {
            tensorData[i] = static_cast<fp16_t>(distr(eng));
        }
    } else if (tensorDesc.dtype == TENSOR_DTYPE_INT32) {
        tensor.dataSize = tensor.Numel() * sizeof(int32_t);
        tensor.data = malloc(tensor.dataSize);
        std::uniform_int_distribution<int32_t> distr(randIntMin_, randIntMax_);
        int32_t *tensorData = static_cast<int32_t *>(tensor.data);
        for (int64_t i = 0; i < tensor.Numel(); i++) {
            tensorData[i] = static_cast<int32_t>(distr(eng));
        }
    } else if (tensorDesc.dtype == TENSOR_DTYPE_INT64) {
        tensor.dataSize = tensor.Numel() * sizeof(int64_t);
        tensor.data = malloc(tensor.dataSize);
        std::uniform_int_distribution<int64_t> distr(randLongMin_, randLongMax_);
        int64_t *tensorData = static_cast<int64_t *>(tensor.data);
        for (int64_t i = 0; i < tensor.Numel(); i++) {
            tensorData[i] = static_cast<int64_t>(distr(eng));
        }
    } else if (tensorDesc.dtype == TENSOR_DTYPE_UINT32) {
        tensor.dataSize = tensor.Numel() * sizeof(uint32_t);
        tensor.data = malloc(tensor.dataSize);
        std::uniform_int_distribution<uint32_t> distr(randIntMin_, randIntMax_);
        uint32_t *tensorData = static_cast<uint32_t *>(tensor.data);
        for (int64_t i = 0; i < tensor.Numel(); i++) {
            tensorData[i] = static_cast<uint32_t>(distr(eng));
        }
    } else if (tensorDesc.dtype == TENSOR_DTYPE_INT8) {
        tensor.dataSize = tensor.Numel() * sizeof(int8_t);
        tensor.data = malloc(tensor.dataSize);
        std::uniform_int_distribution<int8_t> distr(randInt8Min_, randInt8Max_);
        int8_t *tensorData = static_cast<int8_t *>(tensor.data);
        for (int64_t i = 0; i < tensor.Numel(); i++) {
            tensorData[i] = static_cast<int8_t>(distr(eng));
        }
    } else {
        MKI_LOG(ERROR) << "dtype not support in CreateHostRandTensor!";
    }

    return tensor;
}

Tensor OpTest::CreateHostTensor(const Tensor &tensorIn)
{
    Tensor tensor;
    tensor.desc = tensorIn.desc;
    tensor.dataSize = tensorIn.dataSize;

    tensor.data = malloc(tensor.dataSize);
    auto ret = memcpy_s(tensor.data, tensor.dataSize, tensorIn.data, tensor.dataSize);
    MKI_LOG_IF(ret != EOK, ERROR) << "CreateHostTensor fail";

    return tensor;
}

Tensor OpTest::CreateHostZeroTensor(const TensorDesc &tensorDesc)
{
    Tensor tensor;
    tensor.desc = tensorDesc;

    std::random_device rd;
    std::default_random_engine eng(rd());
    if (tensorDesc.dtype == TENSOR_DTYPE_FLOAT) {
        tensor.dataSize = tensor.Numel() * sizeof(float);
    } else if (tensorDesc.dtype == TENSOR_DTYPE_FLOAT16) {
        tensor.dataSize = tensor.Numel() * sizeof(fp16_t);
    } else if (tensorDesc.dtype == TENSOR_DTYPE_INT64) {
        tensor.dataSize = tensor.Numel() * sizeof(int64_t);
    } else if (tensorDesc.dtype == TENSOR_DTYPE_INT32) {
        tensor.dataSize = tensor.Numel() * sizeof(int32_t);
    } else if (tensorDesc.dtype == TENSOR_DTYPE_UINT32) {
        tensor.dataSize = tensor.Numel() * sizeof(uint32_t);
    } else if (tensorDesc.dtype == TENSOR_DTYPE_INT8) {
        tensor.dataSize = tensor.Numel() * sizeof(int8_t);
    } else if (tensorDesc.dtype == TENSOR_DTYPE_UINT8) {
        tensor.dataSize = tensor.Numel() * sizeof(uint8_t);
    } else {
        MKI_LOG(ERROR) << "not support";
        return tensor;
    }

    tensor.data = malloc(tensor.dataSize);
    auto ret = memset_s(tensor.data, tensor.dataSize, 0, tensor.dataSize);
    MKI_LOG_IF(ret != EOK, ERROR) << "CreateHostZeroTensor fail";
    return tensor;
}

Tensor OpTest::CreateHostTensorFromFile(const TensorDesc &tensorDesc, const std::string &dataFile)
{
    Tensor tensor;
    tensor.desc = tensorDesc;
    if (tensorDesc.dtype == TENSOR_DTYPE_FLOAT || tensorDesc.dtype == TENSOR_DTYPE_INT32) {
        tensor.dataSize = tensor.Numel() * sizeof(float);
    } else if (tensorDesc.dtype == TENSOR_DTYPE_FLOAT16) {
        tensor.dataSize = tensor.Numel() * sizeof(fp16_t);
    } else if (tensorDesc.dtype == TENSOR_DTYPE_INT64) {
        tensor.dataSize = tensor.Numel() * sizeof(int64_t);
    } else if (tensorDesc.dtype == TENSOR_DTYPE_UINT32) {
        tensor.dataSize = tensor.Numel() * sizeof(uint32_t);
    } else {
        MKI_LOG(ERROR) << "not support";
        return tensor;
    }

    tensor.data = malloc(tensor.dataSize);
    memset(tensor.data, 0, tensor.dataSize);
    ReadFile(tensor.data, tensor.dataSize, dataFile);

    return tensor;
}

Tensor OpTest::HostTensor2DeviceTensor(const Tensor &hostTensor)
{
    Tensor deviceTensor;
    deviceTensor.desc = hostTensor.desc;
    deviceTensor.dataSize = hostTensor.dataSize;
    int st = MkiRtMemMallocDevice(&deviceTensor.data, deviceTensor.dataSize, MKIRT_MEM_DEFAULT);
    if (st != 0) {
        MKI_LOG(ERROR) << "malloc device tensor fail";
        return deviceTensor;
    }
    st = MkiRtMemCopy(deviceTensor.data, deviceTensor.dataSize, hostTensor.data, hostTensor.dataSize,
                      MKIRT_MEMCOPY_HOST_TO_DEVICE);
    if (st != 0) {
        MKI_LOG(ERROR) << "copy host tensor to device tensor";
    }
    return deviceTensor;
}

void OpTest::Init()
{
    MKI_LOG(INFO) << "MkiRtDeviceSetCurrent " << deviceId_;
    int ret = MkiRtDeviceSetCurrent(deviceId_);
    MKI_LOG_IF(ret != 0, ERROR) << "MkiRtDeviceSetCurrent fail";

    MkiRtStream stream = nullptr;
    MKI_LOG(INFO) << "MkiRtStreamCreate call";
    ret = MkiRtStreamCreate(&stream, 0);
    MKI_LOG_IF(ret != 0, ERROR) << "MkiRtStreamCreate fail";

    runInfo_.SetStream(stream);
}

void OpTest::AddWorkspace()
{
    size_t bufferSize = op_->GetWorkspaceSize();
    if (bufferSize == 0) {
        MKI_LOG(INFO) << "no workspace";
        return;
    }
    MKI_LOG(INFO) << "Workspace size: " << bufferSize;
    uint8_t *deviceBuffer = nullptr;
    int ret = MkiRtMemMallocDevice(reinterpret_cast<void **>(&deviceBuffer), bufferSize, MKIRT_MEM_DEFAULT);
    if (ret != MKIRT_SUCCESS) {
        MKI_LOG(ERROR) << "MkiRtMemMallocDevice fail, errCode:" << ret << ", errName:" << MkiRtErrorName(ret)
                        << "errDesc:" << MkiRtErrorDesc(ret);
        return;
    }
    runInfo_.SetWorkspaceDeviceAddr(deviceBuffer);
}

void OpTest::FreeWorkspace()
{
    uint8_t *deviceBuffer = runInfo_.GetWorkspaceDeviceAddr();
    size_t bufferSize = op_->GetWorkspaceSize();
    if (bufferSize == 0) {
        return;
    }
    if (deviceBuffer != nullptr) {
        MkiRtMemFreeDevice(deviceBuffer);
    }
}

void OpTest::Cleanup()
{
    for (auto tensor : goldenContext_.hostInTensors) {
        if (tensor.data) {
            free(tensor.data);
            tensor.data = nullptr;
        }
    }
    goldenContext_.hostInTensors.clear();

    for (auto tensor : goldenContext_.hostOutTensors) {
        if (tensor.data) {
            free(tensor.data);
            tensor.data = nullptr;
        }
    }
    goldenContext_.hostOutTensors.clear();

    for (auto tensor : goldenContext_.deviceInTensors) {
        if (tensor.data) {
            MkiRtMemFreeDevice(tensor.data);
        }
    }
    goldenContext_.deviceInTensors.clear();

    size_t i = 0;
    for (auto tensor : goldenContext_.deviceOutTensors) {
        if (outUseInputdata_.find(i++) != outUseInputdata_.end()) {
            continue;
        }
        if (tensor.data) {
            MkiRtMemFreeDevice(tensor.data);
        }
    }
    goldenContext_.deviceOutTensors.clear();

    MkiRtStream stream = runInfo_.GetStream();
    if (stream) {
        MkiRtStreamDestroy(stream);
    }

    launchParam_.Reset();
    runInfo_.Reset();
}

OpTestStatistic OpTest::GetRunStatistic() const { return statistic_; }

void OpTest::ReadFile(void *data, size_t dataSize, const std::string &dataFile)
{
    std::ifstream input(dataFile, std::ios::binary);
    if (!input.is_open()) {
        MKI_LOG(ERROR) << "Failed to open file " << dataFile;
        return;
    }

    // copies all data into buffer
    std::vector<unsigned char> buffer(std::istreambuf_iterator<char>(input), {});
    if (buffer.size() != dataSize) {
        MKI_FLOG_ERROR("File size %zu is invalid, expect %zu", buffer.size(), dataSize);
        return;
    }
    MKI_FLOG_INFO("File %s read, total size %zu", dataFile.c_str(), buffer.size());
    auto ret = memcpy_s(data, dataSize, buffer.data(), dataSize);
    MKI_LOG_IF(ret != EOK, ERROR) << "ReadFile fail";
}

std::string OpTest::TensorToString(const Tensor &tensor)
{
    const int64_t printMaxCount = 10;
    std::ostringstream ss;
    ss << "dtype:" << GetStrWithDType(tensor.desc.dtype) << ", format:" << GetStrWithFormat(tensor.desc.format)
       << ", numel:" << tensor.Numel() << ", dataSize:" << tensor.dataSize << ", data:[";

    if (tensor.data) {
        for (int64_t i = 0; i < tensor.Numel(); ++i) {
            if (i == printMaxCount) {
                ss << "...";
                break;
            }

            if (tensor.desc.dtype == TENSOR_DTYPE_FLOAT16) {
                fp16_t *tensorData = static_cast<fp16_t *>(tensor.data);
                ss << tensorData[i] << ",";
            } else if (tensor.desc.dtype == TENSOR_DTYPE_FLOAT) {
                float *tensorData = static_cast<float *>(tensor.data);
                ss << tensorData[i] << ",";
            } else if (tensor.desc.dtype == TENSOR_DTYPE_INT32) {
                int32_t *tensorData = static_cast<int32_t *>(tensor.data);
                ss << tensorData[i] << ",";
            } else if (tensor.desc.dtype == TENSOR_DTYPE_INT64) {
                int64_t *tensorData = static_cast<int64_t *>(tensor.data);
                ss << tensorData[i] << ",";
            } else if (tensor.desc.dtype == TENSOR_DTYPE_INT8) {
                int8_t *tensorData = static_cast<int8_t *>(tensor.data);
                ss << static_cast<int>(tensorData[i]) << ",";
            } else if (tensor.desc.dtype == TENSOR_DTYPE_UINT32) {
                uint32_t *tensorData = static_cast<uint32_t *>(tensor.data);
                ss << tensorData[i] << ",";
            } else {
                ss << "N,";
            }
        }
    } else {
        ss << "null";
    }

    ss << "]";
    return ss.str();
}
} // namespace Mki
